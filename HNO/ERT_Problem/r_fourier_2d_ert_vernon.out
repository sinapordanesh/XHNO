Starting run at: Thu 14 Nov 2024 07:15:13 PM EST
/home/sinpo/repos/XNO/HNO/ERT_Problem/./ERT_fourier.py:477: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load('best_modelV1.pth'))
torch.Size([80000, 40, 100])
torch.Size([80000, 40, 100])
torch.Size([100, 40, 100])
torch.Size([100, 40, 100])
torch.Size([100, 40, 100])
torch.Size([100, 40, 100])
40
100
999937
0 24.876760818995535 0.2308583133816719 0.11106645584106445
1 21.308800232596695 0.1258106071770191 0.06816203832626343
2 21.41502538602799 0.11308150118887425 0.0639391803741455
3 21.60312487743795 0.10273081820905208 0.08220489740371704
4 21.204813809134066 0.09643811716735363 0.06292475223541259
5 21.51135643478483 0.09471108781099319 0.08884321689605713
6 21.375476947054267 0.08971628840863705 0.050968434810638424
7 21.478522478602827 0.08204161441028118 0.06605020761489869
8 21.44012519903481 0.08845818284153938 0.043713431358337405
9 21.456714866682887 0.08493582192957401 0.07021100759506226
10 21.401306395418942 0.07898688360750675 0.039743698835372925
11 21.377452899701893 0.07951655415594577 0.0573256778717041
12 21.33278029691428 0.07507400786876678 0.10800138473510743
13 21.162716146558523 0.07973347448408603 0.03835528612136841
14 21.404300158843398 0.06939794190227985 0.04741540193557739
15 21.416579033248127 0.07504911500662566 0.05164322376251221
16 21.352861882187426 0.06966366892307997 0.057787220478057864
17 21.53729894477874 0.06532294480502605 0.05333241939544678
18 21.55767853371799 0.06939710003435612 0.04444571018218994
19 21.507627237588167 0.06267107560485602 0.09390716791152955
20 21.367652432061732 0.06812748451828957 0.04456906795501709
21 21.52112643327564 0.06273307333737611 0.04127144813537598
22 21.309233811683953 0.06640581992417574 0.039124670028686526
23 22.21636457182467 0.058065778329968454 0.07533019542694092
24 21.58918011840433 0.0652255136847496 0.03747106432914734
25 22.21244996599853 0.058171115343272684 0.03351226568222046
26 21.47334375232458 0.05756582423299551 0.11671600818634033
27 21.29225346725434 0.05810726479440927 0.11626642227172851
28 21.510926944203675 0.056423366487026214 0.06678123712539673
29 21.47239858098328 0.05937409388720989 0.035582410097122194
30 21.363505287095904 0.05636682722270489 0.037403044700622556
31 21.608211236074567 0.05451984891146421 0.03199882626533508
32 21.393764009699225 0.05437236966341734 0.0486141848564148
33 21.392231361940503 0.052332179234921936 0.079669508934021
34 21.43012454919517 0.05888628735989332 0.036065328121185306
35 21.39907561801374 0.05168136005550623 0.057455739974975585
36 21.348438639193773 0.051876769149303434 0.035274683237075805
37 21.307269239798188 0.05025985850095749 0.03099791646003723
38 21.359333656728268 0.0513505613937974 0.03780173301696777
39 21.440446257591248 0.05538193688392639 0.03843826770782471
40 21.66038997936994 0.04727091026604176 0.03263213157653808
41 21.63396267592907 0.04949755041152239 0.03715813636779785
42 21.474184155464172 0.053484726805984976 0.04676438331604004
43 21.326998071745038 0.04794598237723112 0.02940959930419922
44 21.2867360310629 0.04554112048149109 0.037472410202026366
45 21.305598211474717 0.04859790425151587 0.030425349473953246
46 21.363083141855896 0.044778473679721355 0.02780897617340088
47 21.435908067040145 0.0454157390460372 0.03537463307380676
48 21.396503121592104 0.044282284714281556 0.027707290649414063
49 21.601912437938154 0.04482932013124227 0.026450051069259642
50 21.39432250149548 0.04267079406678677 0.031014561653137207
51 21.47545250505209 0.04358677957206965 0.03969549179077148
52 21.3025078792125 0.04119982763975859 0.02879143714904785
53 21.37091082148254 0.041431102952361104 0.025988812446594237
54 21.49105141684413 0.04021335972845554 0.03403513669967651
55 21.386647892184556 0.04027583660334349 0.030909669399261475
56 21.20191274024546 0.03976917272210121 0.027224459648132325
57 21.40306468307972 0.038123934860527516 0.027101024389266967
58 21.40409873239696 0.0382745077997446 0.031480985879898074
59 21.500869976356626 0.03851951832175255 0.025675628185272217
60 21.424683216959238 0.036251609417796135 0.032138911485671995
61 21.919436688534915 0.03629720185995102 0.03152866125106812
62 21.490654719993472 0.03584550271481276 0.026190534830093384
63 21.450797099620104 0.035351687203347686 0.026743173599243164
64 21.432764695957303 0.035023786756396294 0.024254882335662843
65 21.698763025924563 0.033435452112555504 0.031974992752075194
66 21.45457141287625 0.033392854022979736 0.024422111511230468
67 21.596078438684344 0.032574956072866916 0.02291124939918518
68 21.522434500046074 0.031976728270947934 0.022792192697525023
69 21.339424383826554 0.03161004477292299 0.02159261703491211
70 21.478615004569292 0.03074833030551672 0.023420488834381102
71 21.474793454632163 0.030142541106045247 0.021910574436187744
72 21.55781463254243 0.029366493950784205 0.028009064197540283
73 21.451667341403663 0.029067508977651597 0.022481836080551147
74 21.398691914975643 0.028550402790308 0.02009612798690796
75 21.479293443262577 0.028060576970130206 0.020920631885528566
76 21.46752670314163 0.027426666226238013 0.020149300098419188
77 21.388652137480676 0.026994541588425635 0.02127478241920471
78 21.49511214904487 0.026333837121725084 0.021056913137435913
79 21.44006143324077 0.026363750894367696 0.019997580051422117
80 21.801874208264053 0.025486830229312182 0.019938564300537108
81 21.451613229699433 0.02501154203042388 0.018863734006881714
82 21.4704299736768 0.024640503092110157 0.02058076620101929
83 21.754089581780136 0.024168153289705514 0.01868745803833008
84 21.321802030317485 0.02369910753145814 0.018391208052635194
85 21.516509025357664 0.023350040408223867 0.01898194372653961
86 21.44840471353382 0.022911725345253944 0.01829213261604309
87 21.37327552586794 0.02255015177950263 0.019714307188987732
88 21.42476403899491 0.022188952682167293 0.018841623067855834
89 21.32474538590759 0.021903682591021063 0.018035579323768616
90 21.398444027639925 0.02158681611418724 0.018123046159744263
91 21.510890918783844 0.021327978272736074 0.01812645673751831
92 21.38694475032389 0.021087799458950757 0.017554099559783935
93 21.395223540253937 0.020884127566963433 0.017375884652137755
94 21.337275605648756 0.020709111953526736 0.017303122282028197
95 21.521534694358706 0.02056553985401988 0.018079655170440675
96 21.444215540774167 0.020443804039806127 0.01767941892147064
97 21.267171636223793 0.020351792302727698 0.01731629967689514
98 21.39228352997452 0.020287161135673522 0.01730070948600769
99 21.598443254828453 0.02025412604659796 0.017259559035301207
Time taken for predictions on new_test.mat: 0.024263696745038033 seconds
Job finished with exit code 0 at: Thu 14 Nov 2024 07:51:41 PM EST
